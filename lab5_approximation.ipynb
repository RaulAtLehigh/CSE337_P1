{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RaulAtLehigh/CSE337_P1/blob/main/lab5_approximation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 5: Value-Function Approximation\n",
        "\n",
        "\n",
        "## Exercise 1 SGD\n",
        "Many real-world processes can be modeled as nonlinear relationships, and SGD is a standard tool to fit those models from data.  \n",
        "\n",
        "**Example: Predicting energy consumption in a building**  \n",
        "- Energy usage depends on **temperature**, **time of day**, and **occupancy**.  \n",
        "- If you plot the data, the relationship might not be linear — it curves.  \n",
        "- A simple way to approximate this nonlinear relation is to fit a **polynomial function** of temperature (or time).  \n",
        "\n",
        "Now, imagine you’re collecting data continuously:  \n",
        "- You get one data point (temperature, usage) at a time.  \n",
        "- Instead of waiting to collect all data and computing a full batch update, you update your model incrementally with **SGD**.  \n",
        "- This makes your learning **online, adaptive, and scalable** — just like in reinforcement learning.  \n",
        "\n",
        "---\n",
        "\n",
        "In this exercise, approximating a cubic polynomial is a simplified version of **predicting a nonlinear real-world phenomenon**.\n"
      ],
      "metadata": {
        "id": "kXHxamYh1kD0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2lZYncHzhY-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate synthetic data based on a cubic polynomial with Gaussian noise\n",
        "np.random.seed(42)  # for reproducibility\n",
        "n_samples = 10000\n",
        "x = np.linspace(-5, 5, n_samples)\n",
        "# Define the true cubic polynomial function\n",
        "def true_function(x):\n",
        "  return 0.1 * x**3 - 0.5 * x**2 + 2 * x + 5\n",
        "\n",
        "y_true = true_function(x)\n",
        "noise = np.random.normal(0, 2, n_samples)  # Gaussian noise with mean 0 and std dev 2\n",
        "y = y_true + noise\n",
        "# Optional: Plot the generated data\n",
        "plt.scatter(x, y, label='Generated Data')\n",
        "plt.plot(x, y_true, color='red', label='True Function')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.title('Generated Data with Cubic Polynomial and Gaussian Noise')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the given a dataset of input–output pairs `(x, y)` where the underlying relationship is approximately polynomial.  \n",
        "\n",
        "a. Use a **linear model with polynomial features**: Write a function that takes an input `x` and a weight vector `w`, and return the predicted value y_hat.    \n",
        "   \n",
        "   - y_hat(x; w) = w0 + w1 * x + w2 * x^2 + w3 * x^3  \n",
        "\n",
        "   where w0, w1, w2, w3 are the parameters to be learned.\n",
        "\n",
        "b. **Loss function (Mean Squared Error)**:\n",
        "   Write a function that computes the loss for one training point `(x, y)`:\n",
        "\n",
        "     - Formula: `loss = 0.5 * (y - y_hat)**2`\n",
        "\n",
        "c. **SGD update**:   Derive the gradient of the loss with respect to each parameter (w0, w1, w2, w3).  \n",
        "   - Write a function:  \n",
        "\n",
        "     ```python\n",
        "     def sgd_update(x, y, w, alpha):\n",
        "         \"\"\"\n",
        "         Perform one SGD update for a single training example.\n",
        "         Input:\n",
        "             x (float) - input value\n",
        "             y (float) - true output\n",
        "             w (np.array) - current weights\n",
        "             alpha (float) - learning rate\n",
        "         Output:\n",
        "             w (np.array) - updated weights\n",
        "         \"\"\"\n",
        "         # TODO: compute prediction, gradient, and update weights\n",
        "         return w\n",
        "     ```\n",
        "d. **Training loop**:\n",
        "   - Loop over the dataset.  \n",
        "   - At each step, update the weights using `sgd_update`.  \n",
        "   - Track the training loss after each iteration.  \n",
        "\n",
        "e. **Comparison with different learning rates**  \n",
        "   - Train your model using at least three different learning rates, for example:  \n",
        "     - alpha = 0.001  \n",
        "     - alpha = 0.01  \n",
        "     - alpha = 0.1  \n",
        "   - Plot training loss vs iteration for each learning rate.  \n",
        "   - Plot the final fitted polynomial curves for each learning rate on the same graph with the true dataset.\n",
        "\n",
        "---\n",
        "\n",
        "### Deliverables\n",
        "- Code for `predict`, `mse_loss`, and `sgd_update`.  \n",
        "- Training loop that runs with multiple learning rates.  \n",
        "- Plot of training loss vs iteration for each learning rate.  \n",
        "- Plot of the fitted polynomial vs dataset for each learning rate.  \n",
        "- A short discussion:\n",
        "  - How does the learning rate affect convergence speed and stability?  \n",
        "  - Which learning rate gives the best balance between speed and accuracy?  "
      ],
      "metadata": {
        "id": "iTtVnZXS3c5i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## a\n",
        "## x is the feature vector\n",
        "## w is the weights\n",
        "def predict(x, w):\n",
        "  # Using np.float64 for explicit casting, although numpy arrays handle this\n",
        "  return np.float64(w[0]) + np.float64(w[1]) * np.float64(x) + np.float64(w[2]) * np.float64(x)**2 + np.float64(w[3]) * np.float64(x)**3\n",
        "\n",
        "## Loss function (MSE)\n",
        "def mse_loss(y, y_hat):\n",
        "  # Ensure calculations are done in float64 to prevent potential overflow with intermediate results\n",
        "  return np.float64(0.5) * np.float64((y - y_hat))**2\n",
        "\n",
        "def sgd_update(x, y, w, alpha):\n",
        "    \"\"\"\n",
        "    Perform one SGD update for a single training example.\n",
        "    Input:\n",
        "        x (float) - input value\n",
        "        y (float) - true output\n",
        "        w (np.array) - current weights\n",
        "        alpha (float) - learning rate\n",
        "    Output:\n",
        "        w (np.array) - updated weights\n",
        "    \"\"\"\n",
        "    # TODO: compute prediction, gradient, and update weights\n",
        "    y_hat = predict(x, w)\n",
        "\n",
        "    error = np.float64(y - y_hat)\n",
        "\n",
        "    w[0] += np.float64(alpha) * error\n",
        "    w[1] += np.float64(alpha) * error * np.float64(x)\n",
        "    w[2] += np.float64(alpha) * error * np.float64(x)**2\n",
        "    w[3] += np.float64(alpha) * error * np.float64(x)**3\n",
        "    return w\n",
        "\n",
        "## training loop\n",
        "# Using the original learning rates gave me overflow issues, so I'm using these\n",
        "alpha_values = np.array([.0001, .00001, .000001])\n",
        "all_loss = []\n",
        "final_weights = []\n",
        "\n",
        "\n",
        "for alpha in alpha_values:\n",
        "  w = np.zeros(4, dtype=np.float64)\n",
        "  loss = []\n",
        "  for i in range(0, n_samples):\n",
        "    w = sgd_update(x[i], y[i], w, alpha)\n",
        "    current_loss = mse_loss(y[i], predict(x[i], w))\n",
        "    loss.append(current_loss)\n",
        "\n",
        "    if np.isnan(current_loss) or np.isinf(current_loss) or np.any(np.isnan(w)) or np.any(np.isinf(w)):\n",
        "        print(f\"Stopping training for alpha={alpha} due to overflow or NaN at iteration {i}\")\n",
        "        break\n",
        "  all_loss.append(loss)\n",
        "  final_weights.append(w)\n",
        "\n",
        "# Function to calculate moving average\n",
        "def moving_average(data, window_size):\n",
        "    return np.convolve(data, np.ones(window_size), 'valid') / window_size\n",
        "\n",
        "# Plot smoothed loss curves\n",
        "window_size = 100\n",
        "plt.figure(figsize=(10, 6))\n",
        "for i, loss_data in enumerate(all_loss):\n",
        "    if len(loss_data) >= window_size: # Ensure enough data points for moving average\n",
        "      smoothed_loss = moving_average(loss_data, window_size)\n",
        "      plt.plot(smoothed_loss, label=f\"Smoothed Loss alpha = {alpha_values[i]}\")\n",
        "    else:\n",
        "      plt.plot(loss_data, label=f\"Loss alpha = {alpha_values[i]} (not smoothed)\")\n",
        "\n",
        "\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Smoothed Loss over Iterations for different Learning Rates')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Plot the true function and the fitted polynomials\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(x, y, label='Generated Data', alpha=0.5, s=10)\n",
        "plt.plot(x, y_true, color='red', label='True Function')\n",
        "\n",
        "for i, w in enumerate(final_weights):\n",
        "    # Calculate the corresponding y_fitted values using the final weights and the original x for plotting\n",
        "    y_fitted = predict(x, w)\n",
        "    plt.plot(x, y_fitted, label=f'Fitted Polynomial alpha = {alpha_values[i]}')\n",
        "\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.title('Fitted Polynomials vs True Function and Data')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IVryeVVw2i3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**------Discussion:------**\n",
        "1. A higher learning rate results in a faster covergence to a smaller MSE. I do think it results in less stability as data becomes more generalized with each pass, since each step is more significant for in the grand scope of things.\n",
        "2. 1e-05 as the learning rate resulted in the best balance. While I don't get very close to the true function in either, I think this learning rate, has a good convergence rate and looks more similar to the true function."
      ],
      "metadata": {
        "id": "W_f9S2SqaZb-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2: 1000-State Random Walk with Semi-Gradient TD(0)\n",
        "\n",
        "We will study the 1,000-state Random Walk a classic reinforcement learning benchmark from Sutton & Barto.  \n",
        "\n",
        "- The environment has states numbered **1 to 1000**.  \n",
        "- There are two **terminal states**:  \n",
        "  - State `0` on the left (reward = 0)  \n",
        "  - State `1001` on the right (reward = 1)  \n",
        "- Each episode starts in the **middle** at state `500`.  \n",
        "- At each step, the agent moves **left or right with equal probability (0.5 each)**.  \n",
        "- The episode ends when the agent reaches either terminal.  \n",
        "- Discount factor: **γ = 1.0** (episodic task).  \n",
        "\n",
        "\n",
        "### Function Approximation\n",
        "Instead of storing a separate value for each state, approximate the value function with a **linear function of the state index**: V_hat(s; w0, w1) = w0 + w1 * s\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Instructions\n",
        "1. **Implement the environment**:  \n",
        "   - You may **use AI tools such as ChatGPT** to generate the environment code (states, transitions, rewards).  \n",
        "   - Make sure you understand how the environment works.  \n",
        "\n",
        "2. **Implement the TD(0) update manually**:  \n",
        "   - Do **not** use AI for this part.  \n",
        "   - You must write the gradient update equations yourself using the formulas above.  \n",
        "\n",
        "3. **Train your agent**:  \n",
        "   - Run several episodes (e.g., 1000 episodes).  \n",
        "   - Experiment with different step sizes (`alpha`).  \n",
        "\n",
        "4. **Evaluate**:  \n",
        "   - Plot the **true value function** `V*(s) = s/1001`.  \n",
        "   - Plot your **learned approximation line** after training.  \n",
        "   - Discuss whether the line captures the overall trend of the true values.  \n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Guidelines on Using AI Tools\n",
        "- You are encouraged to use AI tools (e.g., ChatGPT, Gemini, Copilot) to **generate helper code**, such as:  \n",
        "  - Building the random walk environment (`reset`, `step`).  \n",
        "  - Implementing the feature map φ(s).  \n",
        "  - Plotting results.  \n",
        "- However, **do not use AI tools to generate the TD(0) update equation**.  \n",
        "  - Deriving and implementing the update is the key learning objective of this exercise.  \n",
        "  - If we find code that uses an AI-generated update without understanding, the score will be zero.  \n",
        "\n",
        "\n",
        "## Deliverables\n",
        "- Python code for the environment and the TD(0) algorithm.  \n",
        "- Plot the **true value function**: For state `s`, the probability of reaching the right terminal is:  \n",
        "  `V*(s) = s / 1001`\n",
        "- Plot comparing the true value function and the approximated line.  \n",
        "- A short discussion:  \n",
        "  - How does the approximation behave for small vs large states?  \n",
        "  - How does the learning rate affect convergence?  \n",
        "\n",
        "---\n",
        "\n",
        "## Hints\n",
        "- Normalize states to `[0,1]` before using them in the line approximation to avoid very large values for w1.  \n",
        "- Start with small step sizes (e.g., 0.001–0.01).  \n",
        "- The approximation will not be perfect (a line cannot match the true curve), but should capture the increasing trend.  \n"
      ],
      "metadata": {
        "id": "n32oJJRFIWqk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class RandomWalkEnvironment:\n",
        "    def __init__(self):\n",
        "        self.n_states = 1000\n",
        "        self.start_state = 500\n",
        "        self.current_state = self.start_state\n",
        "        self.terminal_left = 0\n",
        "        self.terminal_right = 1001\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Resets the environment to the starting state.\"\"\"\n",
        "        self.current_state = self.start_state\n",
        "        return self.current_state\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Takes a step in the environment.\n",
        "\n",
        "        Args:\n",
        "            action (int): 0 for left, 1 for right.\n",
        "\n",
        "        Returns:\n",
        "            tuple: (next_state, reward, done)\n",
        "        \"\"\"\n",
        "        if action == 0: # Move left\n",
        "            self.current_state -= 1\n",
        "        elif action == 1: # Move right\n",
        "            self.current_state += 1\n",
        "        else:\n",
        "            raise ValueError(\"Invalid action\")\n",
        "\n",
        "        # Check for terminal states\n",
        "        done = False\n",
        "        reward = 0\n",
        "        if self.current_state <= self.terminal_left:\n",
        "            self.current_state = self.terminal_left\n",
        "            done = True\n",
        "            reward = 0\n",
        "        elif self.current_state >= self.terminal_right:\n",
        "            self.current_state = self.terminal_right\n",
        "            done = True\n",
        "            reward = 1\n",
        "\n",
        "        return self.current_state, reward, done"
      ],
      "metadata": {
        "id": "zHehiFTNKyxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "# TD(0)\n",
        "def random_policy():\n",
        "  return random.randint(0,1)\n",
        "\n",
        "# the feature vector here should probably just be one dimensional\n",
        "# we can get the most information from the state we're at\n",
        "# # w = weight , x = state\n",
        "# nvm.. instructions say: V_hat(s; w0, w1) = w0 + w1 * s\n",
        "def predict(s, w):\n",
        "  return np.float64(w[0]) + np.float64(w[1]) * np.float64(s)\n",
        "\n",
        "## start the td\n",
        "n_episodes = 1000\n",
        "env = RandomWalkEnvironment()\n",
        "alpha = [.001, .01, .1]\n",
        "w_agg = []\n",
        "\n",
        "for j, alpha_val in enumerate(alpha):\n",
        "  w = [0.0, 0.0] # Initialize w for each alpha value\n",
        "  for ep in range(0, n_episodes):\n",
        "    #print(f\"Starting episode\", {ep + 1})\n",
        "    # initialize S\n",
        "    S = env.reset()\n",
        "    terminal = False\n",
        "    # Loop for each step of episode\n",
        "    while not terminal:\n",
        "      # choose A ~ policy(.|S)\n",
        "      A = random_policy()\n",
        "      # take action A observe R, S'\n",
        "      next_state, reward, terminal = env.step(A)\n",
        "      # w <- w + alpha[R + lambda * v_hat(S',w) - v_hat(S, w)]\n",
        "      # should normalize (left state should probably be 0)\n",
        "      normalized_state = S/1001\n",
        "      error = (reward + predict(next_state, w) - predict(S, w))\n",
        "      #print(j)\n",
        "      w[0] += alpha_val * error\n",
        "      w[1] += alpha_val * error * normalized_state\n",
        "      # S <- S'\n",
        "      S = next_state\n",
        "  w_agg.append(w) # Append the final weights for this alpha\n",
        "  print(f'Completed {n_episodes} episodes for alpha = {alpha_val}')"
      ],
      "metadata": {
        "id": "8SAAxriCR7_w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "947e2424-8a4a-492f-9b92-2e3ce2bcb605"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 1000 episodes for alpha = 0.001\n",
            "Completed 1000 episodes for alpha = 0.01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def true_func(x):\n",
        "  return x /1001\n",
        "\n",
        "true_line = [true_func(i) for i in range(1001)]\n",
        "x_axis = [x for x in range(1001)]\n",
        "\n",
        "plt.plot(x_axis, true_line, label='True Val')\n",
        "for j, alpha_val in enumerate(alpha):\n",
        "  print(w_agg[i])\n",
        "  approx_line = [predict(s, w_agg[i]) for s in range(1001)]\n",
        "  plt.plot(x_axis, approx_line, label=f'Approx. alpha = {alpha_val}')\n",
        "\n",
        "plt.grid(True)\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.legend()\n",
        "plt.title(\"Random Walk Approximation vs True Value\")\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IXJAOMIJAIlv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 3: Solving MountainCar with Tile Coding and SARSA\n",
        "\n",
        "## Problem Description\n",
        "In this exercise, you will solve the **MountainCar-v0** environment from Gym using **tile coding** for function approximation and the **SARSA algorithm** for learning.  \n",
        "\n",
        "The **Mountain Car problem**:\n",
        "- A car is stuck in a valley and is too weak to drive straight up to the goal.  \n",
        "- It must build momentum by going back and forth until it can reach the goal at `position >= 0.5`.  \n",
        "- **State space**: continuous (position, velocity).  \n",
        "- **Actions**: {0: push left, 1: no push, 2: push right}.  \n",
        "- **Reward**: -1 per step until the goal is reached.  \n",
        "- **Episode ends**: when the car reaches the goal or after 200 steps.  \n",
        "\n",
        "---\n",
        "\n",
        "## Step 1: Environment and Random Policy (with AI help)\n",
        "- Use an **AI tool (e.g., ChatGPT)** to generate starter code for:\n",
        "  - Creating the Gym environment (`MountainCar-v0`).  \n",
        "  - Running a **random policy** (actions chosen randomly).  \n",
        "- Run this code to confirm you can interact with the environment and see episode returns.  \n",
        "- This will serve as a **baseline**.  \n",
        "- **Important**: Do not use AI to implement the learning algorithm.  \n",
        "\n",
        "---\n",
        "\n",
        "## Step 2: SARSA Algorithm with Function Approximation\n",
        "Implement **SARSA (on-policy TD control)** with the following steps:\n",
        "\n",
        "For each episode:\n",
        "1. Initialize state `s`.  \n",
        "2. Choose action `a` using **ε-greedy** based on Q(s,a).  \n",
        "3. For each step:  \n",
        "   - Take action `a`, observe `(s_next, r, done)`.  \n",
        "   - Choose next action `a_next` using ε-greedy from `s_next`.  \n",
        "   - Compute TD target:  \n",
        "     ```\n",
        "     target = r + gamma * Q(s_next, a_next)\n",
        "     ```  \n",
        "     (if `s_next` is terminal, then target = r).  \n",
        "   - Compute TD error:  \n",
        "     ```\n",
        "     delta = target - Q(s,a)\n",
        "     ```  \n",
        "   - Update weights:  \n",
        "     ```\n",
        "     w <- w + alpha * delta * x(s,a)\n",
        "     ```  \n",
        "   - Update `s = s_next`, `a = a_next`.  \n",
        "4. End episode when the goal is reached or step limit is hit.  \n",
        "\n",
        "---\n",
        "\n",
        "## Step 3: Experiments\n",
        "- Train the agent for 500–1000 episodes.  \n",
        "- Plot **episode returns (sum of rewards)** vs episodes.  \n",
        "- Compare with the random policy baseline:  \n",
        "  - Does SARSA learn to consistently reach the goal?  \n",
        "  - How many steps does it typically take?  \n",
        "\n",
        "---\n",
        "\n",
        "## Deliverables\n",
        "- Python code for tile coding and SARSA.  \n",
        "- Plot of returns vs episodes.  \n",
        "- Plot the Value function\n",
        "- Short discussion (1–2 paragraphs):  \n",
        "  - Effect of tile coding parameters (number of tilings, resolution).\n"
      ],
      "metadata": {
        "id": "X8ooMF71OBCI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You can use the following code for tiling\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class TileCoderXY:\n",
        "    \"\"\"\n",
        "    A TileCoder for function approximation that applies tile coding on the x and y coordinates\n",
        "    of a 3D state. Instead of providing tile widths, the user provides the number of tiles per\n",
        "    dimension. The tile widths are computed based on the state bounds and the number of tiles.\n",
        "    The z coordinate is not used.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_tilings, tiles_per_dim, state_low, state_high):\n",
        "        \"\"\"\n",
        "        Initialize the TileCoderXY.\n",
        "\n",
        "        Parameters:\n",
        "            num_tilings (int): Number of overlapping tilings.\n",
        "            tiles_per_dim (array-like of 2 ints): Number of tiles along the x and y dimensions.\n",
        "            state_low (array-like of 2 floats): Lower bounds for the x and y dimensions.\n",
        "            state_high (array-like of 2 floats): Upper bounds for the x and y dimensions.\n",
        "        \"\"\"\n",
        "        self.num_tilings = num_tilings\n",
        "        self.tiles_per_dim = np.array(tiles_per_dim, dtype=int)\n",
        "        self.state_low = np.array(state_low, dtype=float)\n",
        "        self.state_high = np.array(state_high, dtype=float)\n",
        "\n",
        "        # Compute the tile width for each dimension.\n",
        "        # We assume that the grid spans exactly from state_low to state_high.\n",
        "        # When there are N tiles, there are N-1 intervals between the boundaries.\n",
        "        self.tile_width = (self.state_high - self.state_low) / (self.tiles_per_dim - 1)\n",
        "\n",
        "        # Precompute an offset for each tiling to create overlapping grids.\n",
        "        # self.offsets = [(i / self.num_tilings) * self.tile_width for i in range(self.num_tilings)]\n",
        "        # self.offsets = self.compute_8_offsets()\n",
        "        # self.offsets = np.stack(self._compute_offsets(), axis=0)  # shape: (num_tilings, dims)\n",
        "        # Precompute offsets for each tiling.\n",
        "        # For tiling i:\n",
        "        #   offset_x = (((i + 0) % num_tilings) / num_tilings) * tile_width[0]\n",
        "        #   offset_y = (((i + 1) % num_tilings) / num_tilings) * tile_width[1]\n",
        "        offsets = np.empty((self.num_tilings, 2))\n",
        "        for i in range(self.num_tilings):\n",
        "            offsets[i, 0] = (((i + 0) % self.num_tilings) / self.num_tilings) * self.tile_width[0]\n",
        "            offsets[i, 1] = (((i + 1) % self.num_tilings) / self.num_tilings) * self.tile_width[1]\n",
        "        self.offsets = offsets\n",
        "\n",
        "\n",
        "        # Precompute multiplier for flattening a 2D index.\n",
        "        # For grid shape (N, M), flat index = x_index * M + y_index.\n",
        "        self.multiplier = self.tiles_per_dim[1]\n",
        "\n",
        "        # Initialize a weight vector for each tiling.\n",
        "        num_tiles = np.prod(self.tiles_per_dim)\n",
        "        self.weights = [np.zeros(num_tiles) for _ in range(self.num_tilings)]\n",
        "\n",
        "    def save(self, file_name):\n",
        "        np.savez(file_name + \".npz\", weights=self.weights)\n",
        "\n",
        "    def load(self, file_name):\n",
        "        self.weights = np.load(file_name+\".npz\")[\"weights\"]\n",
        "\n",
        "\n",
        "    def compute_8_offsets(self):\n",
        "        \"\"\"\n",
        "        Compute a list of offsets using a combination of cardinal and diagonal directions.\n",
        "        The offsets include:\n",
        "          - Center: [0, 0]\n",
        "          - Cardinal: right, left, up, down (half-tile shifts)\n",
        "          - Diagonal: up-right, up-left, down-right, down-left (half-tile shifts)\n",
        "\n",
        "        If the number of tilings exceeds the number of unique offsets (9), the list is repeated.\n",
        "\n",
        "        Returns:\n",
        "            List of 2-element numpy arrays representing the offset for each tiling.\n",
        "        \"\"\"\n",
        "        half_tile = self.tile_width / 8.0\n",
        "        base_offsets = [\n",
        "            np.array([0.0, 0.0]),  # Center (no shift)\n",
        "            np.array([half_tile[0], 0.0]),  # Right\n",
        "            np.array([-half_tile[0], 0.0]),  # Left\n",
        "            np.array([0.0, half_tile[1]]),  # Up\n",
        "            np.array([0.0, -half_tile[1]]),  # Down\n",
        "            np.array([half_tile[0], half_tile[1]]),  # Up-right\n",
        "            np.array([-half_tile[0], half_tile[1]]),  # Up-left\n",
        "            np.array([half_tile[0], -half_tile[1]]),  # Down-right\n",
        "            np.array([-half_tile[0], -half_tile[1]])  # Down-left\n",
        "        ]\n",
        "        offsets = []\n",
        "        for i in range(self.num_tilings):\n",
        "            offsets.append(base_offsets[i % len(base_offsets)])\n",
        "        return offsets\n",
        "\n",
        "    def get_tile_indices(self, state):\n",
        "        \"\"\"\n",
        "        Compute the active tile indices for all tilings given a 2D state.\n",
        "\n",
        "        Parameters:\n",
        "            state (array-like of length 2): The input state [x, y].\n",
        "\n",
        "        Returns:\n",
        "            List of tuples (tiling_index, flat_tile_index) for each tiling.\n",
        "        \"\"\"\n",
        "        state = np.array(state, dtype=float)  # shape: (2,)\n",
        "        # Compute shifted states for all tilings in one vectorized operation.\n",
        "        # Shape of shifted: (num_tilings, 2)\n",
        "        shifted = (state - self.state_low) + self.offsets\n",
        "\n",
        "        # Compute tile coordinates (integer indices) for each tiling.\n",
        "        # Division is broadcasted over the offsets.\n",
        "        tile_coords = (shifted / self.tile_width).astype(int)  # shape: (num_tilings, 2)\n",
        "\n",
        "        # Clip to ensure indices are within bounds.\n",
        "        tile_coords[:, 0] = np.clip(tile_coords[:, 0], 0, self.tiles_per_dim[0] - 1)\n",
        "        tile_coords[:, 1] = np.clip(tile_coords[:, 1], 0, self.tiles_per_dim[1] - 1)\n",
        "\n",
        "        # Compute flat indices for each tiling.\n",
        "        # flat_index = x_index * (tiles_per_dim[1]) + y_index\n",
        "        flat_indices = tile_coords[:, 0] * self.tiles_per_dim[1] + tile_coords[:, 1]\n",
        "\n",
        "        # Return a list of (tiling_index, flat_index) tuples.\n",
        "        return list(zip(range(self.num_tilings), flat_indices))\n",
        "\n",
        "\n",
        "    def predict(self, state):\n",
        "        \"\"\"\n",
        "        Compute the approximated function value for a given 3D state using tile coding on x and y.\n",
        "\n",
        "        Parameters:\n",
        "            state (array-like): The input state [x, y, z].\n",
        "\n",
        "        Returns:\n",
        "            float: The function approximation (sum of weights for the active tiles).\n",
        "        \"\"\"\n",
        "        active_tiles = self.get_tile_indices(state)\n",
        "        return sum(self.weights[tiling][idx] for tiling, idx in active_tiles)\n",
        "\n",
        "    def update(self, state, target, alpha):\n",
        "        \"\"\"\n",
        "        Update the weights given a state and target value.\n",
        "\n",
        "        Parameters:\n",
        "            state (array-like): The input state [x, y, z].\n",
        "            target (float): The target function value.\n",
        "            alpha (float): The overall learning rate.\n",
        "        \"\"\"\n",
        "        prediction = self.predict(state)\n",
        "        error = target - prediction\n",
        "        # Distribute the learning rate equally among all tilings.\n",
        "        alpha_per_tiling = alpha / self.num_tilings\n",
        "\n",
        "        active_tiles = self.get_tile_indices(state)\n",
        "        for tiling, idx in active_tiles:\n",
        "            self.weights[tiling][idx] += alpha_per_tiling * error\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UWX5U5JtOO1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7zkISiB3JRab"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}